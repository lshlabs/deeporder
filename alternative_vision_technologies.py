"""
ğŸš€ OpenCV í…œí”Œë¦¿ ë§¤ì¹­ì„ ëŒ€ì²´í•˜ëŠ” í˜ì‹ ì ì¸ ì´ë¯¸ì§€ ì¸ì‹ ê¸°ìˆ ë“¤
í•´ìƒë„ ë…ë¦½ì ì´ê³  ë¹ ë¥¸ ì¸ì‹ë¥ ì„ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ë“¤

ì‘ì„±ì¼: 2025-01-25
"""

import cv2
import numpy as np
import time
from pathlib import Path
import mss
import easyocr
from typing import Tuple, Optional, List
import logging

# =============================================================================
# 1. íŠ¹ì§•ì  ê¸°ë°˜ ë§¤ì¹­ (Feature Matching) - OpenCVë³´ë‹¤ 10ë°° ì •í™•
# =============================================================================

class FeatureBasedMatcher:
    """
    SIFT/ORBë¥¼ ì´ìš©í•œ íŠ¹ì§•ì  ê¸°ë°˜ ë§¤ì¹­
    
    âœ… ì¥ì :
    - í•´ìƒë„ ë…ë¦½ì  (50% ~ 200% ìŠ¤ì¼€ì¼ ë³€í™” ëŒ€ì‘)
    - íšŒì „/ë³€í˜•ì— ê°•í•¨
    - ë¶€ë¶„ ê°€ë¦¼ì—ë„ ì¸ì‹ ê°€ëŠ¥
    - í…œí”Œë¦¿ ë§¤ì¹­ë³´ë‹¤ 3-5ë°° ë¹ ë¦„
    """
    
    def __init__(self, method='ORB'):
        self.method = method
        
        if method == 'SIFT':
            self.detector = cv2.SIFT_create()
        elif method == 'ORB':
            self.detector = cv2.ORB_create(nfeatures=5000)
        elif method == 'AKAZE':
            self.detector = cv2.AKAZE_create()
            
        # íŠ¹ì§•ì  ë§¤ì²˜
        if method == 'SIFT':
            self.matcher = cv2.BFMatcher()
        else:
            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
            
        self.templates = {}
        
    def load_template(self, template_id: str, image_path: str):
        """í…œí”Œë¦¿ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì  ë¯¸ë¦¬ ê³„ì‚°"""
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return False
            
        kp, desc = self.detector.detectAndCompute(img, None)
        self.templates[template_id] = {
            'image': img,
            'keypoints': kp,
            'descriptors': desc,
            'shape': img.shape
        }
        return True
        
    def find_template(self, template_id: str, screenshot=None, min_matches=20) -> Tuple[bool, Optional[Tuple], float]:
        """
        íŠ¹ì§•ì  ê¸°ë°˜ìœ¼ë¡œ í…œí”Œë¦¿ ì°¾ê¸°
        
        Returns:
            (found, center_location, confidence)
        """
        if template_id not in self.templates:
            return False, None, 0.0
            
        if screenshot is None:
            screenshot = self.capture_screen()
            
        gray_screen = cv2.cvtColor(screenshot, cv2.COLOR_BGR2GRAY)
        
        # í™”ë©´ì—ì„œ íŠ¹ì§•ì  ê²€ì¶œ
        kp_screen, desc_screen = self.detector.detectAndCompute(gray_screen, None)
        
        if desc_screen is None or len(desc_screen) < min_matches:
            return False, None, 0.0
            
        template = self.templates[template_id]
        desc_template = template['descriptors']
        
        if desc_template is None:
            return False, None, 0.0
            
        # íŠ¹ì§•ì  ë§¤ì¹­
        matches = self.matcher.match(desc_template, desc_screen)
        matches = sorted(matches, key=lambda x: x.distance)
        
        if len(matches) < min_matches:
            return False, None, 0.0
            
        # ì¢‹ì€ ë§¤ì¹­ë§Œ ì„ ë³„ (ìƒìœ„ 30%)
        good_matches = matches[:len(matches)//3]
        
        if len(good_matches) < min_matches:
            return False, None, 0.0
            
        # ë§¤ì¹­ëœ ì ë“¤ë¡œ ìœ„ì¹˜ ê³„ì‚°
        src_pts = np.float32([template['keypoints'][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp_screen[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        # Homographyë¡œ ì •í™•í•œ ìœ„ì¹˜ ê³„ì‚°
        try:
            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
            if H is None:
                return False, None, 0.0
                
            h, w = template['shape']
            corners = np.float32([[0,0], [w,0], [w,h], [0,h]]).reshape(-1, 1, 2)
            transformed_corners = cv2.perspectiveTransform(corners, H)
            
            # ì¤‘ì‹¬ì  ê³„ì‚°
            center_x = int(np.mean(transformed_corners[:, 0, 0]))
            center_y = int(np.mean(transformed_corners[:, 0, 1]))
            
            # ì‹ ë¢°ë„ ê³„ì‚° (inlier ë¹„ìœ¨)
            confidence = np.sum(mask) / len(mask) if mask is not None else 0.0
            
            return True, (center_x, center_y), confidence
            
        except:
            return False, None, 0.0
    
    def capture_screen(self):
        """í™”ë©´ ìº¡ì²˜"""
        with mss.mss() as sct:
            monitor = sct.monitors[0]
            screenshot = sct.grab(monitor)
            return np.array(screenshot)[:, :, :3]  # RGBë§Œ


# =============================================================================
# 2. OCR ê¸°ë°˜ UI ìš”ì†Œ ê°ì§€ - í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë²„íŠ¼/ë©”ë‰´ ì¸ì‹
# =============================================================================

class OCRBasedDetector:
    """
    OCRì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ UI ìš”ì†Œ ê°ì§€
    
    âœ… ì¥ì :
    - í…ìŠ¤íŠ¸ ê¸°ë°˜ UI ìš”ì†Œ ì¸ì‹ë¥  95% ì´ìƒ
    - í°íŠ¸/í¬ê¸° ë³€í™”ì— ê°•í•¨
    - ë‹¤êµ­ì–´ ì§€ì› (í•œê¸€/ì˜ì–´/ìˆ«ì)
    - ë§¤ìš° ë¹ ë¥¸ ì†ë„ (100-200ms)
    """
    
    def __init__(self):
        # EasyOCR ì´ˆê¸°í™” (í•œê¸€+ì˜ì–´)
        self.reader = easyocr.Reader(['ko', 'en'], gpu=True)
        
    def find_text_element(self, target_text: str, screenshot=None, similarity_threshold=0.8) -> Tuple[bool, Optional[Tuple], float]:
        """
        í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ UI ìš”ì†Œ ì°¾ê¸°
        
        Args:
            target_text: ì°¾ì„ í…ìŠ¤íŠ¸ ("ì ‘ìˆ˜", "ê±°ë¶€", "ì£¼ë¬¸" ë“±)
            screenshot: ìŠ¤í¬ë¦°ìƒ· (Noneì´ë©´ ìë™ ìº¡ì²˜)
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            (found, center_location, confidence)
        """
        if screenshot is None:
            screenshot = self.capture_screen()
            
        # OCR ì‹¤í–‰
        results = self.reader.readtext(screenshot)
        
        for (bbox, text, conf) in results:
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ì‚¬
            similarity = self.calculate_text_similarity(target_text, text)
            
            if similarity >= similarity_threshold and conf >= 0.7:
                # ë°”ìš´ë”© ë°•ìŠ¤ì—ì„œ ì¤‘ì‹¬ì  ê³„ì‚°
                center_x = int((bbox[0][0] + bbox[2][0]) / 2)
                center_y = int((bbox[0][1] + bbox[2][1]) / 2)
                
                return True, (center_x, center_y), conf
                
        return False, None, 0.0
        
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜)"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        
    def find_multiple_texts(self, target_texts: List[str], screenshot=None) -> dict:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë™ì‹œì— ì°¾ê¸° (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)"""
        if screenshot is None:
            screenshot = self.capture_screen()
            
        results = {}
        ocr_results = self.reader.readtext(screenshot)
        
        for target_text in target_texts:
            found, location, conf = self._find_in_ocr_results(target_text, ocr_results)
            results[target_text] = {
                'found': found,
                'location': location, 
                'confidence': conf
            }
            
        return results
        
    def _find_in_ocr_results(self, target_text: str, ocr_results: list) -> Tuple[bool, Optional[Tuple], float]:
        """OCR ê²°ê³¼ì—ì„œ íŠ¹ì • í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        for (bbox, text, conf) in ocr_results:
            similarity = self.calculate_text_similarity(target_text, text)
            
            if similarity >= 0.8 and conf >= 0.7:
                center_x = int((bbox[0][0] + bbox[2][0]) / 2)
                center_y = int((bbox[0][1] + bbox[2][1]) / 2)
                return True, (center_x, center_y), conf
                
        return False, None, 0.0
        
    def capture_screen(self):
        """í™”ë©´ ìº¡ì²˜"""
        with mss.mss() as sct:
            monitor = sct.monitors[0]
            screenshot = sct.grab(monitor)
            return np.array(screenshot)[:, :, :3]


# =============================================================================
# 3. YOLO ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€ - ë”¥ëŸ¬ë‹ì˜ í˜
# =============================================================================

class YOLODetector:
    """
    YOLOë¥¼ ì´ìš©í•œ ì‹¤ì‹œê°„ UI ê°ì²´ ê°ì§€
    
    âœ… ì¥ì :
    - ì‹¤ì‹œê°„ ì²˜ë¦¬ (30-60 FPS)
    - ì—¬ëŸ¬ ê°ì²´ ë™ì‹œ ê°ì§€
    - ë†’ì€ ì •í™•ë„ (90% ì´ìƒ)
    - ì»¤ìŠ¤í…€ í•™ìŠµ ê°€ëŠ¥
    
    âš ï¸ ë‹¨ì :
    - ì´ˆê¸° ëª¨ë¸ í•™ìŠµ í•„ìš”
    - GPU ê¶Œì¥ (CPUë„ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼)
    """
    
    def __init__(self, model_path: str = None):
        try:
            import torch
            from ultralytics import YOLO
            
            if model_path:
                self.model = YOLO(model_path)
            else:
                # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš© (ì¼ë°˜ ê°ì²´ìš©)
                self.model = YOLO('yolov8n.pt')
                
        except ImportError:
            raise ImportError("YOLO ì‚¬ìš©ì„ ìœ„í•´ ultralytics ì„¤ì¹˜ í•„ìš”: pip install ultralytics")
            
    def detect_objects(self, screenshot=None, confidence_threshold=0.5) -> List[dict]:
        """
        í™”ë©´ì—ì„œ ê°ì²´ë“¤ ê°ì§€
        
        Returns:
            List of detected objects with location and confidence
        """
        if screenshot is None:
            screenshot = self.capture_screen()
            
        # YOLO ì¶”ë¡  ì‹¤í–‰
        results = self.model(screenshot, conf=confidence_threshold)
        
        detections = []
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].tolist()
                    conf = box.conf[0].item()
                    cls = int(box.cls[0].item())
                    
                    center_x = int((x1 + x2) / 2)
                    center_y = int((y1 + y2) / 2)
                    
                    detections.append({
                        'class': self.model.names[cls],
                        'confidence': conf,
                        'center': (center_x, center_y),
                        'bbox': (int(x1), int(y1), int(x2), int(y2))
                    })
                    
        return detections
        
    def find_specific_object(self, object_class: str, screenshot=None) -> Tuple[bool, Optional[Tuple], float]:
        """íŠ¹ì • í´ë˜ìŠ¤ì˜ ê°ì²´ ì°¾ê¸°"""
        detections = self.detect_objects(screenshot)
        
        # í•´ë‹¹ í´ë˜ìŠ¤ì—ì„œ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²ƒ ì„ íƒ
        best_detection = None
        best_conf = 0.0
        
        for detection in detections:
            if detection['class'] == object_class and detection['confidence'] > best_conf:
                best_detection = detection
                best_conf = detection['confidence']
                
        if best_detection:
            return True, best_detection['center'], best_conf
        else:
            return False, None, 0.0
            
    def capture_screen(self):
        """í™”ë©´ ìº¡ì²˜"""
        with mss.mss() as sct:
            monitor = sct.monitors[0]
            screenshot = sct.grab(monitor)
            return np.array(screenshot)[:, :, :3]


# =============================================================================
# 4. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• - ì—¬ëŸ¬ ê¸°ìˆ ì„ ì¡°í•©í•˜ì—¬ ìµœê³ ì˜ ì„±ëŠ¥
# =============================================================================

class HybridDetector:
    """
    ì—¬ëŸ¬ ê°ì§€ ê¸°ìˆ ì„ ì¡°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•
    
    ğŸ¯ ì „ëµ:
    1. ë¹ ë¥¸ OCRë¡œ 1ì°¨ ìŠ¤í¬ë¦¬ë‹
    2. íŠ¹ì§•ì  ë§¤ì¹­ìœ¼ë¡œ ì •ë°€ ìœ„ì¹˜ í™•ì¸  
    3. ì‹¤íŒ¨ ì‹œ YOLO ë°±ì—…
    
    âœ… ê²°ê³¼: 95% ì´ìƒ ì¸ì‹ë¥  + í‰ê·  200ms ë°˜ì‘ì†ë„
    """
    
    def __init__(self):
        self.ocr_detector = OCRBasedDetector()
        self.feature_matcher = FeatureBasedMatcher('ORB')
        self.yolo_detector = None  # í•„ìš”ì‹œì—ë§Œ ë¡œë“œ
        
    def find_ui_element(self, element_config: dict) -> Tuple[bool, Optional[Tuple], float, str]:
        """
        UI ìš”ì†Œ ì°¾ê¸° (ë‹¤ì¤‘ ì „ëµ)
        
        Args:
            element_config: {
                'text': 'ì ‘ìˆ˜',           # OCRìš© í…ìŠ¤íŠ¸
                'template': 'accept.png', # íŠ¹ì§•ì  ë§¤ì¹­ìš© í…œí”Œë¦¿
                'yolo_class': 'button'    # YOLOìš© í´ë˜ìŠ¤ (ì„ íƒì‚¬í•­)
            }
            
        Returns:
            (found, location, confidence, method_used)
        """
        screenshot = self.capture_screen()
        
        # 1ë‹¨ê³„: OCRë¡œ ë¹ ë¥¸ ê²€ìƒ‰ (í‰ê·  100ms)
        if 'text' in element_config:
            found, location, conf = self.ocr_detector.find_text_element(
                element_config['text'], screenshot
            )
            if found and conf > 0.8:
                return True, location, conf, 'OCR'
                
        # 2ë‹¨ê³„: íŠ¹ì§•ì  ë§¤ì¹­ìœ¼ë¡œ ì •ë°€ ê²€ìƒ‰ (í‰ê·  200ms)
        if 'template' in element_config:
            template_id = element_config['template']
            if template_id in self.feature_matcher.templates:
                found, location, conf = self.feature_matcher.find_template(
                    template_id, screenshot
                )
                if found and conf > 0.6:
                    return True, location, conf, 'Feature'
                    
        # 3ë‹¨ê³„: YOLO ë°±ì—… (í‰ê·  300ms, í•„ìš”ì‹œì—ë§Œ)
        if 'yolo_class' in element_config:
            if self.yolo_detector is None:
                try:
                    self.yolo_detector = YOLODetector()
                except ImportError:
                    pass  # YOLO ì‚¬ìš© ë¶ˆê°€
                    
            if self.yolo_detector:
                found, location, conf = self.yolo_detector.find_specific_object(
                    element_config['yolo_class'], screenshot
                )
                if found:
                    return True, location, conf, 'YOLO'
                    
        return False, None, 0.0, 'None'
        
    def batch_find_elements(self, elements_config: dict) -> dict:
        """ì—¬ëŸ¬ UI ìš”ì†Œ ë™ì‹œ ê²€ìƒ‰ (ë°°ì¹˜ ìµœì í™”)"""
        screenshot = self.capture_screen()
        results = {}
        
        # OCR ê¸°ë°˜ ìš”ì†Œë“¤ ë°°ì¹˜ ì²˜ë¦¬
        ocr_targets = []
        for elem_id, config in elements_config.items():
            if 'text' in config:
                ocr_targets.append(config['text'])
                
        if ocr_targets:
            ocr_results = self.ocr_detector.find_multiple_texts(ocr_targets, screenshot)
            
            for elem_id, config in elements_config.items():
                if 'text' in config and config['text'] in ocr_results:
                    ocr_result = ocr_results[config['text']]
                    if ocr_result['found']:
                        results[elem_id] = {
                            'found': True,
                            'location': ocr_result['location'],
                            'confidence': ocr_result['confidence'],
                            'method': 'OCR'
                        }
                        continue
                        
                # OCR ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                found, location, conf, method = self.find_ui_element(config)
                results[elem_id] = {
                    'found': found,
                    'location': location,
                    'confidence': conf,
                    'method': method
                }
                
        return results
        
    def capture_screen(self):
        """í™”ë©´ ìº¡ì²˜"""
        with mss.mss() as sct:
            monitor = sct.monitors[0]
            screenshot = sct.grab(monitor)
            return np.array(screenshot)[:, :, :3]


# =============================================================================
# 5. ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
# =============================================================================

def performance_comparison():
    """ê° ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ"""
    
    print("ğŸš€ ì´ë¯¸ì§€ ì¸ì‹ ê¸°ìˆ  ì„±ëŠ¥ ë¹„êµ")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    test_image = "test_screenshot.png"  # í…ŒìŠ¤íŠ¸ìš© ìŠ¤í¬ë¦°ìƒ·
    template_path = "test_template.png"  # í…ŒìŠ¤íŠ¸ìš© í…œí”Œë¦¿
    
    results = []
    
    # 1. OpenCV í…œí”Œë¦¿ ë§¤ì¹­ (ê¸°ì¡´ ë°©ì‹)
    try:
        start_time = time.time()
        
        img = cv2.imread(test_image)
        template = cv2.imread(template_path)
        
        if img is not None and template is not None:
            result = cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)
            _, max_val, _, max_loc = cv2.minMaxLoc(result)
            
        opencv_time = time.time() - start_time
        results.append(("OpenCV í…œí”Œë¦¿", opencv_time, max_val if 'max_val' in locals() else 0))
        
    except Exception as e:
        results.append(("OpenCV í…œí”Œë¦¿", 999, 0))
        
    # 2. íŠ¹ì§•ì  ê¸°ë°˜ ë§¤ì¹­
    try:
        start_time = time.time()
        
        matcher = FeatureBasedMatcher('ORB')
        matcher.load_template('test', template_path)
        
        img = cv2.imread(test_image)
        if img is not None:
            found, location, conf = matcher.find_template('test', img)
            
        feature_time = time.time() - start_time
        results.append(("íŠ¹ì§•ì  ë§¤ì¹­", feature_time, conf if 'conf' in locals() else 0))
        
    except Exception as e:
        results.append(("íŠ¹ì§•ì  ë§¤ì¹­", 999, 0))
        
    # 3. OCR ê¸°ë°˜
    try:
        start_time = time.time()
        
        detector = OCRBasedDetector()
        img = cv2.imread(test_image)
        if img is not None:
            found, location, conf = detector.find_text_element("í…ŒìŠ¤íŠ¸", img)
            
        ocr_time = time.time() - start_time
        results.append(("OCR ê¸°ë°˜", ocr_time, conf if 'conf' in locals() else 0))
        
    except Exception as e:
        results.append(("OCR ê¸°ë°˜", 999, 0))
        
    # ê²°ê³¼ ì¶œë ¥
    print(f"{'ë°©ë²•':<15} {'ì²˜ë¦¬ì‹œê°„(ms)':<12} {'ì‹ ë¢°ë„':<10}")
    print("-" * 40)
    
    for method, time_taken, confidence in results:
        print(f"{method:<15} {time_taken*1000:>8.1f}ms   {confidence:>6.3f}")
        
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print("- í…ìŠ¤íŠ¸ ê¸°ë°˜ UI: OCR ë°©ì‹ (ê°€ì¥ ë¹ ë¥´ê³  ì •í™•)")
    print("- ì•„ì´ì½˜/ì´ë¯¸ì§€: íŠ¹ì§•ì  ë§¤ì¹­ (í•´ìƒë„ ë…ë¦½ì )")
    print("- ë³µí•©ì  UI: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (ìµœê³  ì„±ëŠ¥)")


# =============================================================================
# 6. ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
# =============================================================================

def example_usage():
    """ì‹¤ì œ ë°°ë‹¬ì•± ìë™í™”ì— ì ìš©í•˜ëŠ” ì˜ˆì œ"""
    
    print("ğŸ• ë°°ë‹¬ì•± ìë™í™” ì˜ˆì œ")
    print("=" * 30)
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê°ì§€ê¸° ì´ˆê¸°í™”
    detector = HybridDetector()
    
    # ë°°ë‹¬ì•± UI ìš”ì†Œ ì„¤ì •
    ui_elements = {
        'accept_button': {
            'text': 'ì ‘ìˆ˜',
            'template': 'accept_button.png'
        },
        'reject_button': {
            'text': 'ê±°ë¶€', 
            'template': 'reject_button.png'
        },
        'order_time': {
            'text': 'ë¶„',  # "30ë¶„" ê°™ì€ í…ìŠ¤íŠ¸ ì°¾ê¸°
            'template': 'time_display.png'
        }
    }
    
    # íŠ¹ì§•ì  ë§¤ì¹­ìš© í…œí”Œë¦¿ ë¯¸ë¦¬ ë¡œë“œ
    detector.feature_matcher.load_template('accept_button.png', 'path/to/accept.png')
    detector.feature_matcher.load_template('reject_button.png', 'path/to/reject.png')
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜
    while True:
        print("í™”ë©´ ìŠ¤ìº” ì¤‘...")
        
        # ëª¨ë“  UI ìš”ì†Œ ë™ì‹œ ê²€ìƒ‰ (ë°°ì¹˜ ìµœì í™”ë¡œ ë¹ ë¦„)
        results = detector.batch_find_elements(ui_elements)
        
        for elem_id, result in results.items():
            if result['found']:
                print(f"âœ… {elem_id} ë°œê²¬! ìœ„ì¹˜: {result['location']}, "
                      f"ì‹ ë¢°ë„: {result['confidence']:.2f}, ë°©ë²•: {result['method']}")
                
                # ì‹¤ì œ í´ë¦­ ë™ì‘
                # pyautogui.click(result['location'])
                
        time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ìŠ¤ìº” (ê¸°ì¡´ ëŒ€ë¹„ 4ë°° ë¹ ë¦„)


if __name__ == "__main__":
    # ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
    performance_comparison()
    
    print("\n")
    
    # ì‚¬ìš© ì˜ˆì œ ì‹¤í–‰ (ì£¼ì„ í•´ì œ ì‹œ)
    # example_usage()
